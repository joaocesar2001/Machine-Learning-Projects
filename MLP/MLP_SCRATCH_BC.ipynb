{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rede neural aplicada a analise de sinal e ruido do méson Bc no canal tauônico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Primeira célula\n",
    "\n",
    "1) Nesta etapa inicializamos os dados de ROOT (framework utilizado pelo CERN em c++) utilizando a biblioteca uproot\n",
    "2) Função de inicialização dos dados. Como tratamos de q^2, BcMass e BcEnergy, load_process se certifica de eliminar estruturas mais complexas do que escalares dentro dos arrays\n",
    "3) Normaliza os dados (StandardScaler usa o z-mean score)\n",
    "4) Separa os dados em teste (20%) e treino (80%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ab0462d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carrega D:\\Projetos de programacao\\Projetos Python\\ML\\Meson_BC\\signal com 462407 eventos e peso 2162.597019508788\n",
      "Carrega D:\\Projetos de programacao\\Projetos Python\\ML\\Meson_BC\\ck com 38386 eventos e peso 26051.16448705257\n",
      "\n",
      "Dados prontos:\n",
      "X_train shape: (400634, 3)\n",
      "y_train shape: (400634,)\n",
      "w_train shape: (400634,)\n"
     ]
    }
   ],
   "source": [
    "import uproot                                               # Usado para abrir arquivos .root\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler            # Importante para normalizar\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "import sklearn.metrics as mt\n",
    "import seaborn as sns\n",
    "\n",
    "                                                            # Função de carregamento \n",
    "def load_process(fIn, variables, target=0, weight_sf=1.):\n",
    "    f = uproot.open(fIn)                                    # abre o arquivo .root\n",
    "    tree = f[\"events\"]                                      # acessa a tree \"events\"\n",
    "    weight = 1.0 / tree.num_entries * weight_sf             # normaliza os pesos \n",
    "    print(f\"Carrega {fIn} com {tree.num_entries} eventos e peso {weight}\")\n",
    "\n",
    "    arrs = tree.arrays(variables, library=\"np\")             # Dados do arquivo .root serão array do numpy, devolve um dicionário com key: branche(variable) e value: array\n",
    "    df = pd.DataFrame({\n",
    "        key: val for key, val in arrs.items()\n",
    "        if isinstance(val, np.ndarray) and val.ndim == 1 and not isinstance(val[0], (list, dict, np.ndarray)) # somente arrays no value, e só escalares no array\n",
    "    })\n",
    "\n",
    "    df[\"target\"] = target\n",
    "    df[\"weight\"] = weight\n",
    "    return df\n",
    "\n",
    "                                                            # Definir variáveis e carregar dados\n",
    "variables = [\"q2\", \"BcMass\", \"BcEnergy\"]\n",
    "signal_path = \"D:\\\\Projetos de programacao\\\\Projetos Python\\\\ML\\\\Meson_BC\\\\signal\"\n",
    "bkg_path    = \"D:\\\\Projetos de programacao\\\\Projetos Python\\\\ML\\\\Meson_BC\\\\ck\"\n",
    "\n",
    "weight_sf = 1e9                                             # Usando o mesmo fator de normalização de peso\n",
    "sig_df = load_process(signal_path, variables, target=1, weight_sf=weight_sf)\n",
    "bkg_df = load_process(bkg_path, variables, target=0, weight_sf=weight_sf)\n",
    "\n",
    "                                                            # Junta tudo num dataframe com os sinais, ruidos e pesos\n",
    "data = pd.concat([sig_df, bkg_df], ignore_index=True)\n",
    "X_data = data[variables].to_numpy()\n",
    "y_data = data[\"target\"].to_numpy()\n",
    "w_data = data[\"weight\"].to_numpy()\n",
    "\n",
    "                                                            #  Normalizar os dados\n",
    "                                                            # A rede usa tanh, que satura rápido com valores grandes\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_data_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "w_data = w_data / np.mean(w_data)\n",
    "                                                            # Dividir em Treino e Teste (80% treino, 20% teste)\n",
    "X_train, X_test, y_train, y_test, w_train, w_test = train_test_split(\n",
    "    X_data_scaled, y_data, w_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_data                                         # Mantém a proporção de sinal/background (em muitos casos há muito mais sig do que bkg)\n",
    ")\n",
    "\n",
    "print(f\"\\nDados prontos:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"w_train shape: {w_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5619c255",
   "metadata": {},
   "source": [
    "## Segunda Célula\n",
    "    Definimos a classe e seus elementos\n",
    "\n",
    "1) _init__ -> Instância os atributos da classe\n",
    "2) forward -> Alimenta as redes (e suas respectivas funções de ativação) com os dados\n",
    "3) Loss -> Quantifica o desempenho da rede\n",
    "4) backpropagation -> Método utilizado para cálculos que realizam o aprendizado da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6908737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NnModel:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray, w: np.ndarray, hidden_neurons: int = 10, output_neurons: int = 1):\n",
    "        np.random.seed(8)\n",
    "        self.x = x\n",
    "                                                            # Garantir que y e w sejam vetores coluna\n",
    "        self.y = y.reshape(-1, 1) \n",
    "        self.w = w.reshape(-1, 1)\n",
    "        \n",
    "        self.hidden_neurons = hidden_neurons\n",
    "                                                            #  Output_neurons é 1 (para classificação binária)\n",
    "        self.output_neurons = output_neurons \n",
    "        self.input_neurons = self.x.shape[1]\n",
    "\n",
    "                                                            # Inicialização de Xavier\n",
    "        self.W1 = np.random.randn(self.input_neurons, self.hidden_neurons) / np.sqrt(self.input_neurons)\n",
    "        self.B1 = np.zeros((1, self.hidden_neurons))\n",
    "        self.W2 = np.random.randn(self.hidden_neurons, self.output_neurons) / np.sqrt(self.hidden_neurons)\n",
    "        self.B2 = np.zeros((1, self.output_neurons))\n",
    "        \n",
    "        self.z1 = 0\n",
    "        self.f1 = 0\n",
    "\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.z1 = x.dot(self.W1) + self.B1\n",
    "        self.f1 = np.tanh(self.z1)                          # Ativação Tanh referente a primeira camada, vai de -1 a 1 centrada em zero, combina com StandardScaler usado\n",
    "        z2 = self.f1.dot(self.W2) + self.B2                 \n",
    "        z2_stable = np.clip(z2, -250, 250)                  # fixa os valores grandes e pequenos nesse limite pra função não explodir\n",
    "        sigmoid = 1 / (1 + np.exp(-z2_stable))              # Ativação referente a camada de output\n",
    "        return sigmoid\n",
    "\n",
    "        \n",
    "                                                            # Loss -> Binary Cross-Entropy ponderada\n",
    "    def loss(self, sigmoid):\n",
    "        num_samples = self.y.shape[0]\n",
    "        epsilon = 1e-9                                      # Evita log(0) -> inf\n",
    "    \n",
    "                                                            # Binary Cross-Entropy (BCE), resultados errados penalizam a rede com loss alto\n",
    "        bce = -(self.y * np.log(sigmoid + epsilon) + (1 - self.y) * np.log(1 - sigmoid + epsilon))\n",
    "        \n",
    "                                                            # Aplica pesos ao bce, para que erros raros ou mais frequentes sejam ajustados\n",
    "        weighted_bce = bce * self.w\n",
    "        \n",
    "                                                            # Média do loss ponderado\n",
    "                                                            # Dividido pelo N total, pois os pesos já ajustam a importância\n",
    "        data_loss = np.sum(weighted_bce) / num_samples\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "                                                            # Backpropagation para BCE Ponderada\n",
    "    def backpropagation(self, sigmoid: np.ndarray, learning_rate: float) -> None:\n",
    "        num_samples = self.x.shape[0]\n",
    "\n",
    "                                                            # Derivada da loss function da última camada (cálculo utilizando regra da cadeia)\n",
    "                                                            # É (sigmoid - y_true) * weight\n",
    "        delta2 = (sigmoid - self.y) * self.w\n",
    "\n",
    "                                                            # Gradientes para W2 e B2, mostra os pesos que ajustaram o resultado delta 2 acima\n",
    "        dW2 = (self.f1.T).dot(delta2)\n",
    "        dB2 = np.sum(delta2, axis = 0, keepdims = True)\n",
    "\n",
    "                                                            # Propagar para a camada 1\n",
    "                                                            # Derivada da Tanh\n",
    "        delta1 = delta2.dot(self.W2.T) * (1 - np.power(self.f1, 2))\n",
    "        \n",
    "                                                            # Gradientes para W1 e B1\n",
    "        dW1 = (self.x.T).dot(delta1)\n",
    "        dB1 = np.sum(delta1, axis = 0, keepdims = True)\n",
    "\n",
    "                                                            # Média dos gradientes\n",
    "        dW1 /= num_samples\n",
    "        dB1 /= num_samples\n",
    "        dW2 /= num_samples\n",
    "        dB2 /= num_samples\n",
    "\n",
    "                                                            # Atualização dos pesos\n",
    "        self.W1 += - learning_rate * dW1\n",
    "        self.W2 += - learning_rate * dW2\n",
    "        self.B1 += - learning_rate * dB1\n",
    "        self.B2 += - learning_rate * dB2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs: int, lr: float):\n",
    "                                                            # Loop que passa os dados pra rede dentro de uma época \n",
    "        for epoch in range(epochs):\n",
    "            outputs = self.forward(self.x)                  # Tenta adivinhar\n",
    "            loss = self.loss(outputs)                       # O Loss determina o quão errado a previsão está\n",
    "            \n",
    "            self.backpropagation(outputs, lr)               # Aprende (corrige os pesos) \n",
    "\n",
    "                                                            # Acurácia (convertendo y de (N,1) para (N,) para comparar)\n",
    "            y_flat = self.y.flatten()\n",
    "            \n",
    "                                                            # Predição é > 0.5\n",
    "            prediction = (outputs > 0.5).astype(int).flatten()\n",
    "            \n",
    "            correct = (prediction == y_flat).sum()          # Conta quantas vezes o modelo acertou e divide pelo número total previsões para calcular % \n",
    "            accuracy = correct / y_flat.shape[0]\n",
    "\n",
    "                                                            # Só exibe acurácia e perda em pacotes de 10% do número total de épocas escolhidas\n",
    "            if int((epoch+1) % (epochs/10)) == 0:\n",
    "                print(f\"Epoch: [{epoch + 1} / {epochs}] Accuracy: {accuracy:.3f} Loss: {loss.item():.5f}\")\n",
    "\n",
    "        \n",
    "                                                            # Retorna as predições finais do treino\n",
    "        return (self.forward(self.x) > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8282f7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [40 / 400] Accuracy: 0.700 Loss: 0.43745\n",
      "Epoch: [80 / 400] Accuracy: 0.744 Loss: 0.41402\n",
      "Epoch: [120 / 400] Accuracy: 0.765 Loss: 0.40424\n",
      "Epoch: [160 / 400] Accuracy: 0.775 Loss: 0.39950\n",
      "Epoch: [200 / 400] Accuracy: 0.780 Loss: 0.39692\n",
      "Epoch: [240 / 400] Accuracy: 0.783 Loss: 0.39535\n",
      "Epoch: [280 / 400] Accuracy: 0.784 Loss: 0.39431\n",
      "Epoch: [320 / 400] Accuracy: 0.786 Loss: 0.39356\n",
      "Epoch: [360 / 400] Accuracy: 0.786 Loss: 0.39299\n",
      "Epoch: [400 / 400] Accuracy: 0.786 Loss: 0.39254\n",
      "\n",
      "--- Treinamento Concluído ---\n"
     ]
    }
   ],
   "source": [
    "# Instanciar o NOVO modelo\n",
    "# Foi passado X_train, y_train, e w_train\n",
    "\n",
    "model = NnModel(X_train, y_train, w_train, hidden_neurons=64, output_neurons=1)\n",
    "\n",
    "# Treinar o modelo\n",
    "# Usando 400 épocas e learning rate 0.1\n",
    "\n",
    "result_train = model.fit(400, 0.1)\n",
    "\n",
    "print(\"\\n--- Treinamento Concluído ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c687743",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Fazer as previsões (forward pass) com os dados de TESTE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m test_outputs = \u001b[43mmodel\u001b[49m.forward(X_test)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Pegar a classe ( > 0.5 )\u001b[39;00m\n\u001b[32m      5\u001b[39m test_predictions = (test_outputs > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m).flatten()\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fazer as previsões (forward pass) com os dados de TESTE\n",
    "test_outputs = model.forward(X_test)\n",
    "\n",
    "# Pegar a classe ( > 0.5 )\n",
    "test_predictions = (test_outputs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calcular a acurácia comparando com os rótulos verdadeiros y_test\n",
    "test_accuracy = (test_predictions == y_test).sum() / y_test.shape[0]\n",
    "\n",
    "print(f\"Acurácia no Conjunto de Teste: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plotar a Matriz de Confusão\n",
    "cm = mt.confusion_matrix(y_test, test_predictions) # Sem pesos\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Background (0)', 'Signal (1)'], \n",
    "            yticklabels=['Background (0)', 'Signal (1)'])\n",
    "plt.xlabel('Valor Previsto')\n",
    "plt.ylabel('Valor Verdadeiro')\n",
    "plt.title('Matriz de Confusão - Dados de Teste')\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, test_outputs.flatten(), sample_weight=w_test)\n",
    "roc_auc = auc(fpr, tpr) # Calcula a área sob a curva (AUC)\n",
    "\n",
    "print(f\"\\nAUC (Área sob a Curva ROC): {roc_auc:.4f}\")\n",
    "\n",
    "# Plotando\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Classificador Aleatório (AUC = 0.5)')\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
